# SAiLL AI Engine Configuration
# Copy this file to .env and configure your settings

# =============================================================================
# OpenAI Configuration (Phase 1)
# =============================================================================
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=150
OPENAI_TEMPERATURE=0.7
OPENAI_RATE_LIMIT_RPM=500
OPENAI_RATE_LIMIT_TPM=30000

# =============================================================================
# Local Llama Configuration (Phase 2)
# =============================================================================
LLAMA_MODEL_PATH=meta-llama/Llama-3.1-8B-Instruct
LLAMA_MAX_GPU_MEMORY=14
LLAMA_TARGET_MEMORY_USAGE=0.85
LLAMA_MAX_NEW_TOKENS=150
LLAMA_TEMPERATURE=0.7
LLAMA_CONTEXT_LENGTH=4096

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL connection for conversation logging and client management
DATABASE_URL=postgresql://saill_user:password@localhost:5432/saill_platform
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=30

# Redis for caching and session management
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=
REDIS_MAX_CONNECTIONS=50

# =============================================================================
# Industry Intelligence Configuration
# =============================================================================
INDUSTRY_INTELLIGENCE_TYPE=mock
INDUSTRY_CACHE_TTL_MINUTES=30
INDUSTRY_INTELLIGENCE_TIMEOUT=30

# =============================================================================
# Engine Configuration
# =============================================================================
# Default engine preference: openai, local_llama, hybrid
DEFAULT_ENGINE=hybrid
ENABLE_FAILOVER=true
PREFER_LOCAL_ENGINE=true
ENGINE_HEALTH_CHECK_INTERVAL=300

# =============================================================================
# Performance and Monitoring
# =============================================================================
LOG_LEVEL=INFO
ENABLE_PERFORMANCE_MONITORING=true
ENABLE_PROMETHEUS_METRICS=true
PROMETHEUS_PORT=8090

# Response time targets (milliseconds)
TARGET_RESPONSE_TIME_MS=1000
OPENAI_TARGET_RESPONSE_TIME_MS=2000
LLAMA_TARGET_RESPONSE_TIME_MS=350

# =============================================================================
# Security Configuration
# =============================================================================
# API security
API_KEY_HEADER=X-API-Key
ALLOWED_ORIGINS=["http://localhost:3000", "https://saill.ai"]
ENABLE_CORS=true

# Conversation data encryption
CONVERSATION_ENCRYPTION_KEY=your_32_character_encryption_key_here
ENABLE_CONVERSATION_ENCRYPTION=false

# =============================================================================
# Development and Testing
# =============================================================================
ENVIRONMENT=development
DEBUG_MODE=false
ENABLE_REQUEST_LOGGING=true
ENABLE_RESPONSE_CACHING=true
CACHE_TTL_SECONDS=300

# Testing configuration
PYTEST_ASYNCIO_MODE=auto
TEST_DATABASE_URL=postgresql://saill_test:password@localhost:5432/saill_test
TEST_REDIS_URL=redis://localhost:6379/1

# =============================================================================
# Hardware Optimization
# =============================================================================
# GPU settings for local Llama
CUDA_VISIBLE_DEVICES=0
TORCH_CUDA_ARCH_LIST=8.6
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# CPU settings
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8

# =============================================================================
# Production Deployment
# =============================================================================
# Server configuration
HOST=0.0.0.0
PORT=8000
WORKERS=4
WORKER_CLASS=uvicorn.workers.UvicornWorker
WORKER_CONNECTIONS=1000
MAX_REQUESTS=10000
MAX_REQUESTS_JITTER=1000

# Health check endpoints
HEALTH_CHECK_PATH=/health
METRICS_PATH=/metrics
STATUS_PATH=/status

# =============================================================================
# Client Configuration Defaults
# =============================================================================
DEFAULT_CONVERSATION_TIMEOUT=300
DEFAULT_CONVERSATION_MAX_TURNS=50
DEFAULT_INDUSTRY=general
DEFAULT_SUBSCRIPTION_TIER=standard

# =============================================================================
# Feature Flags
# =============================================================================
ENABLE_HYBRID_ENGINE=true
ENABLE_INDUSTRY_INTELLIGENCE=true
ENABLE_CONVERSATION_ANALYTICS=true
ENABLE_COST_TRACKING=true
ENABLE_QUALITY_MONITORING=true

# Voice integration (future)
ENABLE_VOICE_PIPELINE=false
WHISPER_MODEL=base
TTS_MODEL=tts-1
